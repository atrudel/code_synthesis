{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyCorewar import ToyCorewar\n",
    "from environment import Env\n",
    "from program_synthesis import Program, Instruction\n",
    "from DQN_utils import LinearSchedule\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyCorewar Characteristics\n",
    "NUM_ACTIONS = 225\n",
    "NUM_REGISTERS = 4\n",
    "MAX_LENGTH = 5\n",
    "N_INSTRUCTIONS = 4\n",
    "N_VARS = 3\n",
    "N_VALS = 20\n",
    "N_TARGETS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        \n",
    "        h_size_a = 50\n",
    "        h_size_b = 100\n",
    "        h_size_c = 120\n",
    "        s_size = N_TARGETS * 2\n",
    "        \n",
    "        self.lstm_p_a = nn.LSTM(input_size=N_INSTRUCTIONS, hidden_size=h_size_a, num_layers=2)\n",
    "        self.lstm_p_b = nn.LSTM(input_size=(N_VARS * NUM_REGISTERS), hidden_size=h_size_b, num_layers=2)\n",
    "        self.lstm_p_c = nn.LSTM(input_size=N_VALS, hidden_size=h_size_c, num_layers=2)\n",
    "        self.fc_s1 = nn.Linear(in_features=s_size, out_features=s_size)\n",
    "        self.fc_s2 = nn.Linear(in_features=s_size, out_features=s_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=(h_size_a + h_size_b + h_size_c + s_size), out_features=200)\n",
    "        self.fc2 = nn.Linear(in_features=200, out_features=128)\n",
    "        \n",
    "        self.fc1_adv = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc1_val = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc2_adv = nn.Linear(in_features=128, out_features=NUM_ACTIONS)\n",
    "        self.fc2_val = nn.Linear(in_features=128, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        p_a, p_b, p_c, s = state\n",
    "        \n",
    "        # Process instruction, variable and value embeddings\n",
    "        # in separate streams of 2-layer LSTMs\n",
    "        # Collecting the hidden state\n",
    "        _,(p_a,_) = self.lstm_p_a(p_a.float())\n",
    "        _,(p_b,_) = self.lstm_p_b(p_b.float())\n",
    "        _,(p_c,_) = self.lstm_p_c(p_c.float())\n",
    "        \n",
    "        # Process state vector in 2 FC layers\n",
    "        s = self.relu(self.fc_s1(s.float()))\n",
    "        s = self.relu(self.fc_s2(s.float()))\n",
    "        \n",
    "        # Concatenate P and S vectors and process in 2 FC layers\n",
    "        x = torch.cat((p_a[1], p_b[1], p_c[1], s), dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        # Split processing in 2 streams: value and advantage\n",
    "        adv = self.relu(self.fc1_adv(x))\n",
    "        val = self.relu(self.fc1_val(x))\n",
    "        \n",
    "        adv = self.fc2_adv(adv)\n",
    "        val = self.fc2_val(val).expand(-1, NUM_ACTIONS)\n",
    "        \n",
    "        x = val + adv - adv.mean().expand(NUM_ACTIONS)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tensors(state):\n",
    "    prog_state, mem_state = state\n",
    "    instr, var, val = zip(*prog_state)\n",
    "    instr = torch.tensor(instr).unsqueeze(1)\n",
    "    var = torch.tensor(var).unsqueeze(1)\n",
    "    val = torch.tensor(val).unsqueeze(1)\n",
    "    mem = torch.tensor(mem_state).view(-1).unsqueeze(0)\n",
    "    return instr, var, val, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensors(batch):\n",
    "    tensors = [state_to_tensors(state) for state in batch]\n",
    "    instr_tensors, var_tensors, val_tensors, mem_tensors = zip(*tensors)\n",
    "    instr = torch.cat(instr_tensors, dim=1)\n",
    "    var = torch.cat(var_tensors, dim=1)\n",
    "    val = torch.cat(val_tensors, dim=1)\n",
    "    mem = torch.cat(mem_tensors, dim=0)\n",
    "    return instr, var, val, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling double Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (with hindsight experience replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import inspect\n",
    "import os\n",
    "import time\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "def train(Q, reward_func, M, verbose=False, log_dir=None):\n",
    "\n",
    "    env = Env(reward_func)\n",
    "\n",
    "    replay_buffer_size = 100000\n",
    "    learning_starts = 100\n",
    "    learning_freq = 4\n",
    "    batch_size=32\n",
    "    num_actions = env.action_space_n\n",
    "    \n",
    "    log_freq = 100\n",
    "    save_freq = 1000\n",
    "\n",
    "    gamma = 0.99\n",
    "    epsilon_schedule = LinearSchedule(schedule_timesteps=M, final_p=0.1)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    Q_target = Dueling_DQN()\n",
    "    Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = optim.RMSprop(Q.parameters())\n",
    "    num_parameter_updates = 0\n",
    "    target_update_freq = 1000\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Starting training [reward = {}] for {} episodes...\".format(\n",
    "            reward_func.__name__, M))\n",
    "    \n",
    "    if log_dir is not None:\n",
    "        log_file = os.path.join(log_dir, \"logs\")\n",
    "        with open(log_file, \"w\") as f:\n",
    "            print(\"Starting training for {} episodes...\".format(M), file=f)\n",
    "            print(\"Reward function:\\n\\n{}\\n\\n\\n\".format(inspect.getsource(reward_func)), file=f)\n",
    "        model_dir = os.path.join(log_dir, \"models\")\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for episode in range(M):\n",
    "        s = env.reset()\n",
    "\n",
    "        for t in range(MAX_LENGTH):\n",
    "            # Select action with E-greedy policy\n",
    "            if episode < learning_starts or np.random.rand() < epsilon_schedule.value(episode):\n",
    "                a = np.random.randint(num_actions)\n",
    "            else:\n",
    "                a = Q(state_to_tensors(s)).argmax(1).item()\n",
    "\n",
    "            # Submit chosen action to the environment\n",
    "            s_prime, reward, done, info = env.step(a)\n",
    "\n",
    "            # Store the effect of the action\n",
    "            replay_buffer.append(Transition(s, a, reward, s_prime, done))\n",
    "\n",
    "            # New state becomes current state\n",
    "            s = s_prime\n",
    "\n",
    "            # EXPERIENCE REPLAY\n",
    "            if (episode > learning_starts and episode % learning_freq == 0):\n",
    "                # Sample from the replay buffer\n",
    "                transitions = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "                # Extract each batch of elements from the sample of transitions\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                state_batch = batch_to_tensors(batch.state)\n",
    "                action_batch = torch.LongTensor(batch.action).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(batch.reward, dtype=torch.float)\n",
    "                next_state_batch = batch_to_tensors(batch.next_state)\n",
    "                done_batch = torch.tensor(batch.done, dtype=torch.float)\n",
    "\n",
    "                # Get the current network's estimations for the q-values of all (state, action)\n",
    "                # pairs in the batch\n",
    "                q_s_a = Q(state_batch).gather(1, action_batch).squeeze()\n",
    "\n",
    "                # Calculate the corresponding target q-values to send to the loss function\n",
    "                a_prime =  Q(next_state_batch).argmax(1).unsqueeze(1)\n",
    "                q_s_a_prime = Q_target(next_state_batch).gather(1, a_prime).squeeze()\n",
    "                q_s_a_prime *= 1 - done_batch\n",
    "                target_q_s_a = reward_batch + gamma * q_s_a_prime\n",
    "                target_q_s_a = target_q_s_a.detach()\n",
    "\n",
    "                # Backprop\n",
    "                loss = loss_function(q_s_a, target_q_s_a)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                num_parameter_updates += 1\n",
    "\n",
    "                # Update target DQN every once in a while\n",
    "                if num_parameter_updates % target_update_freq == 0:\n",
    "                    Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Console output\n",
    "        if verbose and episode % log_freq == 0:\n",
    "            print(\"Episode {} completed\".format(episode + 1))\n",
    "        \n",
    "        # Log output\n",
    "        if log_dir is not None and episode % log_freq == 0:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                print(\"Episode {}: [time:{}]\\n\".format(episode+1, time.time()-start_time), file=f)\n",
    "                assess(Q, reward_func, file=f)\n",
    "                print(\"\\n\\n\\n\", file=f)\n",
    "                \n",
    "                \n",
    "        # Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program synthesis (after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(Q, reward_func, file=None):\n",
    "    env = Env(reward_func)\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(MAX_LENGTH):\n",
    "        a = Q(state_to_tensors(s)).argmax(1).item()\n",
    "        s_prime, reward, done, info = env.step(a)\n",
    "        s = s_prime\n",
    "        if done:\n",
    "            break\n",
    "    env.print_details(file=file)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific values for all registers, without division\n",
    "def specific_register_values(cw):\n",
    "    target_values = np.array([0,10,10,20], dtype=int)\n",
    "    reward = 0\n",
    "    for reg in range(N_TARGETS):\n",
    "        reward -= abs(target_values[reg] - cw.registers[reg])\n",
    "    return cw.registers, target_values, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific values for all registers, with division\n",
    "def specific_register_values_division(cw):\n",
    "    target_values = np.array([0,10,10,20], dtype=int)\n",
    "    reward = 0\n",
    "    for reg in range(N_TARGETS):\n",
    "        reward -= abs((target_values[reg] - cw.registers[reg]) / (target_values[reg] + 1))\n",
    "    return cw.registers, target_values, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific value for one register\n",
    "def one_register_value(cw):\n",
    "    target = 55\n",
    "    register = 3 # Reminder: register indexes start from 1\n",
    "    target_values = np.zeros(4, dtype=int)\n",
    "    target_values[register-1] = target\n",
    "    reward = -abs(target - cw.registers[register-1])\n",
    "    return cw.registers, target_values, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize the sum of all register values\n",
    "def maximize_all_registers(cw):\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    reward = cw.registers.sum()\n",
    "    return cw.registers, target, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize the sum of all register values\n",
    "def minimize_all_registers(cw):\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    reward = -cw.registers.sum()\n",
    "    return cw.registers, target, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_functions = [specific_register_values,\n",
    "                   specific_register_values_division,\n",
    "                   one_register_value,\n",
    "                   maximize_all_registers,\n",
    "                   minimize_all_registers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training [reward = minimize_all_registers] for 2000 episodes...\n",
      "Episode 1 completed\n",
      "Episode 101 completed\n",
      "Episode 201 completed\n",
      "Episode 301 completed\n",
      "Episode 401 completed\n",
      "Episode 501 completed\n",
      "Episode 601 completed\n",
      "Episode 701 completed\n",
      "Episode 801 completed\n",
      "Episode 901 completed\n",
      "Episode 1001 completed\n",
      "Episode 1101 completed\n",
      "Episode 1201 completed\n",
      "Episode 1301 completed\n",
      "Episode 1401 completed\n",
      "Episode 1501 completed\n",
      "Episode 1601 completed\n",
      "Episode 1701 completed\n",
      "Episode 1801 completed\n",
      "Episode 1901 completed\n",
      "sub 3  3  3    \t[  0  0  0  0 ]     0\n",
      "sub 3  3  3    \t[  0  0  0  0 ]     0\n",
      "sub 3  3  3    \t[  0  0  0  0 ]     0\n",
      "sub 3  3  3    \t[  0  0  0  0 ]     0\n",
      "sub 3  3  3    \t[  0  0  0  0 ]     0\n",
      "-------------------------------------\n",
      "                      Total reward: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<program_synthesis.Program at 0x617064a58>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = Dueling_DQN()\n",
    "train(Q, minimize_all_registers, 2000, verbose=True)\n",
    "assess(Q, minimize_all_registers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(reward_func, episodes, root_dir):\n",
    "    log_dir = os.path.join(root_dir, reward_func.__name__)\n",
    "    os.makedirs(log_dir)\n",
    "    Q = Dueling_DQN()\n",
    "    train(Q, reward_func, episodes, log_dir=log_dir)\n",
    "    final_save_path = os.path.join(log_dir, \"models\", \"final\")\n",
    "    torch.save(Q.state_dict(), final_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_series(name, reward_functions, episodes):\n",
    "    os.makedirs(\"Experiments\", exist_ok=True)\n",
    "    root_dir = os.path.join(\"Experiments\", name)\n",
    "    os.makedirs(root_dir)\n",
    "    if isinstance(episodes, int):\n",
    "        episodes = [episodes] * len(reward_functions)\n",
    "    for reward_func, ep in zip(reward_functions, episodes):\n",
    "        run_experiment(reward_func, ep, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_series(\"helloWorld3\", reward_functions, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
